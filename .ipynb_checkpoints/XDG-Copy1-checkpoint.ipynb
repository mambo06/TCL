{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd6157f-c103-4ac1-9b4f-dfee82785fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import json\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d094f18-e780-44fe-a00e-a36d3e8a7286",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [\n",
    "             'adult',\n",
    "             'aloi',\n",
    "             'california_housing',\n",
    "             'covtype',\n",
    "             'epsilon',\n",
    "             'helena',\n",
    "             'higgs_small',\n",
    "             'jannis',\n",
    "             'microsoft',\n",
    "             'yahoo',\n",
    "             'year'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c41f23c1-9c65-4c16-a7a9-c20a34e32fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinData(dbName, cat_policy='ohe',seed=int(9),normalization=False, norm=\"l1\", id=True ):\n",
    "        dataset_name = dbName\n",
    "        dir_ = Path('data/'+ dataset_name )\n",
    "        y_train = np.load(dir_.joinpath('y_train.npy'))\n",
    "        y_test = np.load(dir_.joinpath('y_test.npy'))\n",
    "        y_val = np.load(dir_.joinpath('y_val.npy'))\n",
    "        # y = np.concatenate((y_train,y_test,y_val), axis=0)\n",
    "        y = [y_train,y_test,y_val]\n",
    "        \n",
    "        if dir_.joinpath('C_train.npy').exists() and not id:\n",
    "            C_train = np.load(dir_.joinpath('C_train.npy'))\n",
    "            C_test = np.load(dir_.joinpath('C_test.npy'))\n",
    "            C_val = np.load(dir_.joinpath('C_val.npy'))\n",
    "            # C = np.concatenate((C_train,C_test,C_val), axis=0)\n",
    "            \n",
    "            ord = OrdinalEncoder()\n",
    "            C_train = ord.fit_transform(C_train)\n",
    "            C_test = ord.transform(C_test)\n",
    "            C_val = ord.transform(C_val)\n",
    "            C = [C_train,C_test,C_val]\n",
    "            \n",
    "            \n",
    "            if cat_policy == 'indices':\n",
    "                C = C\n",
    "            elif cat_policy == 'ohe':\n",
    "                ohe = sklearn.preprocessing.OneHotEncoder(\n",
    "                    handle_unknown='ignore', sparse=False, dtype='float32'  # type: ignore[code]\n",
    "                )\n",
    "                ohe.fit(C[0])\n",
    "                C[0] = ohe.transform(C[0])\n",
    "                C[1] = ohe.transform(C[1])\n",
    "                C[2] = ohe.transform(C[2])\n",
    "            elif cat_policy == 'counter':\n",
    "                assert seed is not None\n",
    "                loo = LeaveOneOutEncoder(sigma=0.1, random_state=seed, return_df=False)\n",
    "                loo.fit(C[0], y[0])\n",
    "                C[0] = loo.transform(C[0])  # type: ignore[code]\n",
    "                C[1] = loo.transform(C[1])\n",
    "                C[2] = loo.transform(C[2])\n",
    "            result = C\n",
    "                    \n",
    "        if dir_.joinpath('N_train.npy').exists():\n",
    "            N_train = np.load(dir_.joinpath('N_train.npy'))\n",
    "            N_test = np.load(dir_.joinpath('N_test.npy'))\n",
    "            N_val = np.load(dir_.joinpath('N_val.npy'))\n",
    "            # N = np.concatenate((N_train,N_test,N_val), axis=0)\n",
    "            N = [N_train,N_test,N_val]\n",
    "            # print('size :',N_test.shape, N_val.shape)\n",
    "            result = N\n",
    "            \n",
    "        if ('N' in locals()) and ('C' in locals()):\n",
    "            result[0] = np.concatenate((C[0],N[0]), axis=1)\n",
    "            result[1] = np.concatenate((C[1],N[1]), axis=1)\n",
    "            result[2] = np.concatenate((C[2],N[2]), axis=1)\n",
    "        #dropna\n",
    "        a = ~np.isnan(result[0]).any(axis=1)\n",
    "        result[0] = result[0][a]\n",
    "        y[0] = y[0][a]\n",
    "        a = ~np.isnan(result[1]).any(axis=1)\n",
    "        result[1] = result[1][a]\n",
    "        y[1] = y[1][a]\n",
    "        a = ~np.isnan(result[2]).any(axis=1)\n",
    "        result[2] = result[2][a]\n",
    "        y[2] = y[2][a]\n",
    "        if normalization:\n",
    "            mmx = MinMaxScaler()\n",
    "            result[0] = mmx.fit_transform(result[0])\n",
    "            result[2] = mmx.transform(result[2])\n",
    "\n",
    "            result[1] = mmx.transform(result[1])\n",
    "        \n",
    "        return result[0],result[1],result[2], y[0],y[1],y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f454625a-f7c9-4700-903a-514800f823c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datbase used : adult\n",
      "Test score: precision.      0.6225690887843323, recall 0.6558600331373522, F1 0.5914561737473192, support None\n",
      "datbase used : aloi\n"
     ]
    }
   ],
   "source": [
    "for dbs in dirs:\n",
    "    print('datbase used :',dbs)\n",
    "    config = {}\n",
    "    config['task_type'] = json.loads(Path('data/'+dbs+'/info.json').read_text())['task_type']\n",
    "    config['cat_policy'] = json.loads(Path('data/'+dbs+'/info.json').read_text())['cat_policy']\n",
    "    config['norm'] = json.loads(Path('data/'+dbs+'/info.json').read_text())['norm']\n",
    "    dir_ = 'data/'+ dbs\n",
    "    N_train, N_test,N_val, y_train, y_test,y_val = joinData(dbs,\n",
    "                                                            cat_policy=config['cat_policy'],\n",
    "                                                            normalization=True, \n",
    "                                                            norm=config['norm'])\n",
    "    train_data = lgb.Dataset(N_train, label=y_train)\n",
    "    test_data = lgb.Dataset(N_test, label=y_test, reference=train_data)\n",
    "    # Define hyperparameters\n",
    "    \n",
    "    # Train the LightGBM model\n",
    "    num_round = 500\n",
    "    if config['task_type']  != 'regression':\n",
    "        params = {\n",
    "        \t# \"objective\": \"binary\",\n",
    "            'objective': 'multiclass',\n",
    "            'num_class': len(set(y_train)),\n",
    "        \t\"boosting_type\": \"rf\",\n",
    "        \t\"num_leaves\": 5,\n",
    "        \t\"force_row_wise\": True,\n",
    "        \t\"learning_rate\": 0.5,\n",
    "        \t# \"metric\": \"binary_logloss\",\n",
    "        \t\"bagging_fraction\": 0.8,\n",
    "        \t\"feature_fraction\": 0.8,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "    else :\n",
    "        params = { \n",
    "        \t'objective': 'regression', \n",
    "        \t'metric': 'rmse', \n",
    "        \t'boosting_type': 'gbdt', \n",
    "        \t'num_leaves': 31, \n",
    "        \t'learning_rate': 0.05, \n",
    "        \t'feature_fraction': 0.9,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        bst = lgb.LGBMRegressor(metric='rmse') \n",
    "        bst.fit(N_train, y_train)\n",
    "        \n",
    "    y_hat_test = bst.predict(N_test)\n",
    "    # y_hat_test = (y_hat_test > 0.5).astype(int)\n",
    "    if config['task_type']  != 'regression':\n",
    "        y_hat_test = np.argmax(y_hat_test, axis=1)\n",
    "        te_acc =  precision_recall_fscore_support(y_test, y_hat_test, average='macro')\n",
    "        print(\"Test score: precision.      {}, recall {}, F1 {}, support {}\".format(te_acc[0],te_acc[1],te_acc[2],te_acc[3]) )\n",
    "    else:\n",
    "        te_acc = np.sqrt(mean_squared_error(y_test, y_hat_test)) \n",
    "        print(te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f3316-9ac1-4c09-b5e2-a5b7d493e200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
