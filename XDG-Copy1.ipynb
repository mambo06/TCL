{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cd6157f-c103-4ac1-9b4f-dfee82785fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import json\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d094f18-e780-44fe-a00e-a36d3e8a7286",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [\n",
    "             'adult',\n",
    "             'aloi',\n",
    "             'california_housing',\n",
    "             'covtype',\n",
    "             # 'epsilon',\n",
    "             'helena',\n",
    "             'higgs_small',\n",
    "             'jannis',\n",
    "             'microsoft',\n",
    "             'yahoo',\n",
    "             'year'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c41f23c1-9c65-4c16-a7a9-c20a34e32fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinData(dbName, cat_policy='ohe',seed=int(9),normalization=False, norm=\"l1\", id=True ):\n",
    "        dataset_name = dbName\n",
    "        dir_ = Path('data/'+ dataset_name )\n",
    "        y_train = np.load(dir_.joinpath('y_train.npy'))\n",
    "        y_test = np.load(dir_.joinpath('y_test.npy'))\n",
    "        y_val = np.load(dir_.joinpath('y_val.npy'))\n",
    "        # y = np.concatenate((y_train,y_test,y_val), axis=0)\n",
    "        y = [y_train,y_test,y_val]\n",
    "        \n",
    "        if dir_.joinpath('C_train.npy').exists() and not id:\n",
    "            C_train = np.load(dir_.joinpath('C_train.npy'))\n",
    "            C_test = np.load(dir_.joinpath('C_test.npy'))\n",
    "            C_val = np.load(dir_.joinpath('C_val.npy'))\n",
    "            # C = np.concatenate((C_train,C_test,C_val), axis=0)\n",
    "            \n",
    "            ord = OrdinalEncoder()\n",
    "            C_train = ord.fit_transform(C_train)\n",
    "            C_test = ord.transform(C_test)\n",
    "            C_val = ord.transform(C_val)\n",
    "            C = [C_train,C_test,C_val]\n",
    "            \n",
    "            \n",
    "            if cat_policy == 'indices':\n",
    "                C = C\n",
    "            elif cat_policy == 'ohe':\n",
    "                ohe = sklearn.preprocessing.OneHotEncoder(\n",
    "                    handle_unknown='ignore', sparse=False, dtype='float32'  # type: ignore[code]\n",
    "                )\n",
    "                ohe.fit(C[0])\n",
    "                C[0] = ohe.transform(C[0])\n",
    "                C[1] = ohe.transform(C[1])\n",
    "                C[2] = ohe.transform(C[2])\n",
    "            elif cat_policy == 'counter':\n",
    "                assert seed is not None\n",
    "                loo = LeaveOneOutEncoder(sigma=0.1, random_state=seed, return_df=False)\n",
    "                loo.fit(C[0], y[0])\n",
    "                C[0] = loo.transform(C[0])  # type: ignore[code]\n",
    "                C[1] = loo.transform(C[1])\n",
    "                C[2] = loo.transform(C[2])\n",
    "            result = C\n",
    "                    \n",
    "        if dir_.joinpath('N_train.npy').exists():\n",
    "            N_train = np.load(dir_.joinpath('N_train.npy'))\n",
    "            N_test = np.load(dir_.joinpath('N_test.npy'))\n",
    "            N_val = np.load(dir_.joinpath('N_val.npy'))\n",
    "            # N = np.concatenate((N_train,N_test,N_val), axis=0)\n",
    "            N = [N_train,N_test,N_val]\n",
    "            # print('size :',N_test.shape, N_val.shape)\n",
    "            result = N\n",
    "            \n",
    "        if ('N' in locals()) and ('C' in locals()):\n",
    "            result[0] = np.concatenate((C[0],N[0]), axis=1)\n",
    "            result[1] = np.concatenate((C[1],N[1]), axis=1)\n",
    "            result[2] = np.concatenate((C[2],N[2]), axis=1)\n",
    "        #dropna\n",
    "        a = ~np.isnan(result[0]).any(axis=1)\n",
    "        result[0] = result[0][a]\n",
    "        y[0] = y[0][a]\n",
    "        a = ~np.isnan(result[1]).any(axis=1)\n",
    "        result[1] = result[1][a]\n",
    "        y[1] = y[1][a]\n",
    "        a = ~np.isnan(result[2]).any(axis=1)\n",
    "        result[2] = result[2][a]\n",
    "        y[2] = y[2][a]\n",
    "        if normalization:\n",
    "            mmx = MinMaxScaler()\n",
    "            result[0] = mmx.fit_transform(result[0])\n",
    "            result[2] = mmx.transform(result[2])\n",
    "\n",
    "            result[1] = mmx.transform(result[1])\n",
    "        \n",
    "        return result[0],result[1],result[2], y[0],y[1],y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f454625a-f7c9-4700-903a-514800f823c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datbase used : adult\n",
      "Test score: precision.      0.9201136198841102, recall 0.9366947238903531, F1 0.9278977312091548, support None\n"
     ]
    }
   ],
   "source": [
    "for dbs in dirs[:1]:\n",
    "    print('datbase used :',dbs)\n",
    "    param_grid = {\"max_depth\":    [ 8,10,],\n",
    "              \"n_estimators\": [900, 1000],\n",
    "              \"learning_rate\": [0.01, 0.015]}\n",
    "    config = {}\n",
    "    config['task_type'] = json.loads(Path('data/'+dbs+'/info.json').read_text())['task_type']\n",
    "    config['cat_policy'] = json.loads(Path('data/'+dbs+'/info.json').read_text())['cat_policy']\n",
    "    config['norm'] = json.loads(Path('data/'+dbs+'/info.json').read_text())['norm']\n",
    "    dir_ = 'data/'+ dbs\n",
    "    N_train, N_test,N_val, y_train, y_test,y_val = joinData(dbs,\n",
    "                                                            cat_policy=config['cat_policy'],\n",
    "                                                            normalization=True, \n",
    "                                                            norm=config['norm'])\n",
    " \n",
    "    if config['task_type']  != 'regression':\n",
    "        bst = CatBoostClassifier(verbose=False)\n",
    "        bst.fit(N_train, y_train)\n",
    "    else :\n",
    "        \n",
    "        bst = CatBoostRegressor(verbose=False)\n",
    "        \n",
    "        bst.fit(N_train, y_train)\n",
    "        \n",
    "    y_hat_test = bst.predict(N_test)\n",
    "    # y_hat_test = (y_hat_test > 0.5).astype(int)\n",
    "    if config['task_type']  != 'regression':\n",
    "        te_acc =  precision_recall_fscore_support(y_test, y_hat_test, average='macro')\n",
    "        print(\"Test score: precision.      {}, recall {}, F1 {}, support {}\".format(te_acc[0],te_acc[1],te_acc[2],te_acc[3]) )\n",
    "    else:\n",
    "        te_acc = np.sqrt(mean_squared_error(y_test, y_hat_test)) \n",
    "        print(te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f3316-9ac1-4c09-b5e2-a5b7d493e200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
