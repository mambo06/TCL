{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cd6157f-c103-4ac1-9b4f-dfee82785fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import json\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d094f18-e780-44fe-a00e-a36d3e8a7286",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [\n",
    "             'adult',\n",
    "             'aloi',\n",
    "             'california_housing',\n",
    "             'covtype',\n",
    "             # 'epsilon',\n",
    "             'helena',\n",
    "             'higgs_small',\n",
    "             'jannis',\n",
    "             'microsoft',\n",
    "             'yahoo',\n",
    "             'year'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c41f23c1-9c65-4c16-a7a9-c20a34e32fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinData(dbName, cat_policy='ohe',seed=int(9),normalization=False, norm=\"l1\", id=True ):\n",
    "        dataset_name = dbName\n",
    "        dir_ = Path('data/'+ dataset_name )\n",
    "        y_train = np.load(dir_.joinpath('y_train.npy'))\n",
    "        y_test = np.load(dir_.joinpath('y_test.npy'))\n",
    "        y_val = np.load(dir_.joinpath('y_val.npy'))\n",
    "        # y = np.concatenate((y_train,y_test,y_val), axis=0)\n",
    "        y = [y_train,y_test,y_val]\n",
    "        \n",
    "        if dir_.joinpath('C_train.npy').exists() and not id:\n",
    "            C_train = np.load(dir_.joinpath('C_train.npy'))\n",
    "            C_test = np.load(dir_.joinpath('C_test.npy'))\n",
    "            C_val = np.load(dir_.joinpath('C_val.npy'))\n",
    "            # C = np.concatenate((C_train,C_test,C_val), axis=0)\n",
    "            \n",
    "            ord = OrdinalEncoder()\n",
    "            C_train = ord.fit_transform(C_train)\n",
    "            C_test = ord.transform(C_test)\n",
    "            C_val = ord.transform(C_val)\n",
    "            C = [C_train,C_test,C_val]\n",
    "            \n",
    "            \n",
    "            if cat_policy == 'indices':\n",
    "                C = C\n",
    "            elif cat_policy == 'ohe':\n",
    "                ohe = sklearn.preprocessing.OneHotEncoder(\n",
    "                    handle_unknown='ignore', sparse=False, dtype='float32'  # type: ignore[code]\n",
    "                )\n",
    "                ohe.fit(C[0])\n",
    "                C[0] = ohe.transform(C[0])\n",
    "                C[1] = ohe.transform(C[1])\n",
    "                C[2] = ohe.transform(C[2])\n",
    "            elif cat_policy == 'counter':\n",
    "                assert seed is not None\n",
    "                loo = LeaveOneOutEncoder(sigma=0.1, random_state=seed, return_df=False)\n",
    "                loo.fit(C[0], y[0])\n",
    "                C[0] = loo.transform(C[0])  # type: ignore[code]\n",
    "                C[1] = loo.transform(C[1])\n",
    "                C[2] = loo.transform(C[2])\n",
    "            result = C\n",
    "                    \n",
    "        if dir_.joinpath('N_train.npy').exists():\n",
    "            N_train = np.load(dir_.joinpath('N_train.npy'))\n",
    "            N_test = np.load(dir_.joinpath('N_test.npy'))\n",
    "            N_val = np.load(dir_.joinpath('N_val.npy'))\n",
    "            # N = np.concatenate((N_train,N_test,N_val), axis=0)\n",
    "            N = [N_train,N_test,N_val]\n",
    "            # print('size :',N_test.shape, N_val.shape)\n",
    "            result = N\n",
    "            \n",
    "        if ('N' in locals()) and ('C' in locals()):\n",
    "            result[0] = np.concatenate((C[0],N[0]), axis=1)\n",
    "            result[1] = np.concatenate((C[1],N[1]), axis=1)\n",
    "            result[2] = np.concatenate((C[2],N[2]), axis=1)\n",
    "        #dropna\n",
    "        a = ~np.isnan(result[0]).any(axis=1)\n",
    "        result[0] = result[0][a]\n",
    "        y[0] = y[0][a]\n",
    "        a = ~np.isnan(result[1]).any(axis=1)\n",
    "        result[1] = result[1][a]\n",
    "        y[1] = y[1][a]\n",
    "        a = ~np.isnan(result[2]).any(axis=1)\n",
    "        result[2] = result[2][a]\n",
    "        y[2] = y[2][a]\n",
    "        if normalization:\n",
    "            mmx = MinMaxScaler()\n",
    "            result[0] = mmx.fit_transform(result[0])\n",
    "            result[2] = mmx.transform(result[2])\n",
    "\n",
    "            result[1] = mmx.transform(result[1])\n",
    "        \n",
    "        return result[0],result[1],result[2], y[0],y[1],y[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6765a944-ae5f-45b5-916c-cb826dc4c943",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f454625a-f7c9-4700-903a-514800f823c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datbase used : adult\n",
      "Test score: precision.      0.6225690887843323, recall 0.6558600331373522, F1 0.5914561737473192, support None\n",
      "datbase used : aloi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ag/RQ3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ag/RQ3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: precision.      0.21337026197614348, recall 0.21606678877003418, F1 0.177869353242904, support None\n",
      "datbase used : california_housing\n",
      "0.8486248532967058\n",
      "datbase used : covtype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ag/RQ3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: precision.      0.34490742642544564, recall 0.35247988106086714, F1 0.21988081145214844, support None\n",
      "datbase used : epsilon\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask_type\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     19\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m     \t\u001b[38;5;66;03m# \"objective\": \"binary\",\u001b[39;00m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     31\u001b[0m     }\n\u001b[0;32m---> 32\u001b[0m     bst \u001b[38;5;241m=\u001b[39m \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_round\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[1;32m     34\u001b[0m     params \u001b[38;5;241m=\u001b[39m { \n\u001b[1;32m     35\u001b[0m     \t\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     36\u001b[0m     \t\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     42\u001b[0m     }\n",
      "File \u001b[0;32m~/RQ3/lib/python3.10/site-packages/lightgbm/engine.py:307\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    296\u001b[0m     cb(\n\u001b[1;32m    297\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[1;32m    298\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m         )\n\u001b[1;32m    305\u001b[0m     )\n\u001b[0;32m--> 307\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/RQ3/lib/python3.10/site-packages/lightgbm/basic.py:4136\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   4134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4135\u001b[0m _safe_call(\n\u001b[0;32m-> 4136\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4137\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4140\u001b[0m )\n\u001b[1;32m   4141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   4142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for dbs in dirs:\n",
    "    print('datbase used :',dbs)\n",
    "    config = {}\n",
    "    config['task_type'] = json.loads(Path('data/'+dbs+'/info.json').read_text())['task_type']\n",
    "    config['cat_policy'] = json.loads(Path('data/'+dbs+'/info.json').read_text())['cat_policy']\n",
    "    config['norm'] = json.loads(Path('data/'+dbs+'/info.json').read_text())['norm']\n",
    "    dir_ = 'data/'+ dbs\n",
    "    N_train, N_test,N_val, y_train, y_test,y_val = joinData(dbs,\n",
    "                                                            cat_policy=config['cat_policy'],\n",
    "                                                            normalization=True, \n",
    "                                                            norm=config['norm'])\n",
    "    train_data = lgb.Dataset(N_train, label=y_train)\n",
    "    test_data = lgb.Dataset(N_test, label=y_test, reference=train_data)\n",
    "    # Define hyperparameters\n",
    "    \n",
    "    # Train the LightGBM model\n",
    "    num_round = 500\n",
    "    if config['task_type']  != 'regression':\n",
    "        params = {\n",
    "        \t# \"objective\": \"binary\",\n",
    "            'objective': 'multiclass',\n",
    "            'num_class': len(set(y_train)),\n",
    "        \t\"boosting_type\": \"rf\",\n",
    "        \t\"num_leaves\": 5,\n",
    "        \t\"force_row_wise\": True,\n",
    "        \t\"learning_rate\": 0.5,\n",
    "        \t# \"metric\": \"binary_logloss\",\n",
    "        \t\"bagging_fraction\": 0.8,\n",
    "        \t\"feature_fraction\": 0.8,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "    else :\n",
    "        params = { \n",
    "        \t'objective': 'regression', \n",
    "        \t'metric': 'rmse', \n",
    "        \t'boosting_type': 'gbdt', \n",
    "        \t'num_leaves': 31, \n",
    "        \t'learning_rate': 0.05, \n",
    "        \t'feature_fraction': 0.9,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        bst = lgb.LGBMRegressor(metric='rmse') \n",
    "        bst.fit(N_train, y_train)\n",
    "        \n",
    "    y_hat_test = bst.predict(N_test)\n",
    "    # y_hat_test = (y_hat_test > 0.5).astype(int)\n",
    "    if config['task_type']  != 'regression':\n",
    "        y_hat_test = np.argmax(y_hat_test, axis=1)\n",
    "        te_acc =  precision_recall_fscore_support(y_test, y_hat_test, average='macro')\n",
    "        print(\"Test score: precision.      {}, recall {}, F1 {}, support {}\".format(te_acc[0],te_acc[1],te_acc[2],te_acc[3]) )\n",
    "    else:\n",
    "        te_acc = np.sqrt(mean_squared_error(y_test, y_hat_test)) \n",
    "        print(te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef5f3316-9ac1-4c09-b5e2-a5b7d493e200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datbase used : helena\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ag/RQ3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ag/RQ3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: precision.      0.09993238633035897, recall 0.10637597538293235, F1 0.08065018997701773, support None\n",
      "datbase used : higgs_small\n",
      "Test score: precision.      0.615909090909091, recall 0.6162393162393163, F1 0.6095707248416609, support None\n",
      "datbase used : jannis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ag/RQ3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: precision.      0.42774186770184963, recall 0.4383964545769754, F1 0.4321501807972209, support None\n",
      "datbase used : microsoft\n",
      "0.7407009826492306\n",
      "datbase used : yahoo\n",
      "0.6613803240834467\n",
      "datbase used : year\n",
      "6.565720357897446\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0027e503-38fb-4e29-aa2b-27ada04ae9b5",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1432147-b003-4dda-8794-be91650e82e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datbase used : adult\n",
      "Test score: precision.      0.9278552909186026, recall 0.9227481106815528, F1 0.925256983992242, support None\n",
      "datbase used : aloi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ag/RQ3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ag/RQ3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: precision.      0.3706611547609482, recall 0.39433934004740284, F1 0.328221534567486, support None\n",
      "datbase used : california_housing\n",
      "0.84528166\n",
      "datbase used : covtype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ag/RQ3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: precision.      0.7388901174504889, recall 0.6864699377831568, F1 0.7009150559227958, support None\n",
      "datbase used : helena\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ag/RQ3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: precision.      0.16740193479560309, recall 0.12284424369203932, F1 0.12725910560235512, support None\n",
      "datbase used : higgs_small\n",
      "Test score: precision.      0.7414084507042253, recall 0.7441595441595441, F1 0.7392857142857143, support None\n",
      "datbase used : jannis\n",
      "Test score: precision.      0.6068562489222572, recall 0.5253594736143012, F1 0.5320718438953862, support None\n",
      "datbase used : microsoft\n",
      "0.73995155\n",
      "datbase used : yahoo\n",
      "0.65408266\n",
      "datbase used : year\n",
      "6.867605\n"
     ]
    }
   ],
   "source": [
    "for dbs in dirs:\n",
    "    print('datbase used :',dbs)\n",
    "    # param_grid = {\"max_depth\":    [ 8,10,],\n",
    "    #           \"n_estimators\": [900, 1000],\n",
    "    #           \"learning_rate\": [0.01, 0.015]}\n",
    "    config = {}\n",
    "    config['task_type'] = json.loads(Path('data/'+dbs+'/info.json').read_text())['task_type']\n",
    "    config['cat_policy'] = json.loads(Path('data/'+dbs+'/info.json').read_text())['cat_policy']\n",
    "    config['norm'] = json.loads(Path('data/'+dbs+'/info.json').read_text())['norm']\n",
    "    dir_ = 'data/'+ dbs\n",
    "    N_train, N_test,N_val, y_train, y_test,y_val = joinData(dbs,\n",
    "                                                            cat_policy=config['cat_policy'],\n",
    "                                                            normalization=True, \n",
    "                                                            norm=config['norm'])\n",
    " \n",
    "    if config['task_type']  != 'regression':\n",
    "        bst = xgb.XGBClassifier(\n",
    "                           # learning_rate = param_grid[\"learning_rate\"][-1],\n",
    "                           # n_estimators  = param_grid[\"n_estimators\"][-1],\n",
    "                           # max_depth     = param_grid[\"max_depth\"][-1],\n",
    "                           verbosity = 0)\n",
    "        bst.fit(N_train, y_train)\n",
    "    else :\n",
    "        \n",
    "        bst = xgb.XGBRegressor(\n",
    "                               # learning_rate = param_grid[\"learning_rate\"][-1],\n",
    "                               # n_estimators  = param_grid[\"n_estimators\"][-1],\n",
    "                               # max_depth     = param_grid[\"max_depth\"][-1],\n",
    "                               verbosity = 0)\n",
    "        \n",
    "        bst.fit(N_train, y_train)\n",
    "        \n",
    "    y_hat_test = bst.predict(N_test)\n",
    "    # y_hat_test = (y_hat_test > 0.5).astype(int)\n",
    "    if config['task_type']  != 'regression':\n",
    "        # y_hat_test = np.argmax(y_hat_test, axis=1)\n",
    "        te_acc =  precision_recall_fscore_support(y_test, y_hat_test, average='macro')\n",
    "        print(\"Test score: precision.      {}, recall {}, F1 {}, support {}\".format(te_acc[0],te_acc[1],te_acc[2],te_acc[3]) )\n",
    "    else:\n",
    "        te_acc = np.sqrt(mean_squared_error(y_test, y_hat_test)) \n",
    "        print(te_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ef853-bd6f-425d-8525-e36a8dfa1e77",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80b1fbd1-ef12-47a3-93f4-51ec71446db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datbase used : adult\n",
      "Test score: precision.      0.9201136198841102, recall 0.9366947238903531, F1 0.9278977312091548, support None\n",
      "datbase used : california_housing\n",
      "0.8279809598489288\n",
      "datbase used : covtype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ag/RQ3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: precision.      0.7540582411876368, recall 0.7578008578659762, F1 0.7533530379848016, support None\n",
      "datbase used : helena\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ag/RQ3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: precision.      0.20405737859273043, recall 0.14519831976895115, F1 0.15223633035905038, support None\n",
      "datbase used : higgs_small\n",
      "Test score: precision.      0.71996996996997, recall 0.7226020892687559, F1 0.7185310575069356, support None\n",
      "datbase used : jannis\n",
      "Test score: precision.      0.6134877673721948, recall 0.5267033742752648, F1 0.5334201821413607, support None\n",
      "datbase used : microsoft\n",
      "0.7332631697625569\n",
      "datbase used : yahoo\n",
      "0.6557508836889767\n",
      "datbase used : year\n",
      "6.622122774033987\n"
     ]
    }
   ],
   "source": [
    "for dbs in dirs:\n",
    "    if dbs == 'aloi' : continue\n",
    "    print('datbase used :',dbs)\n",
    "    param_grid = {\"max_depth\":    [ 8,10,],\n",
    "              \"n_estimators\": [900, 1000],\n",
    "              \"learning_rate\": [0.01, 0.015]}\n",
    "    config = {}\n",
    "    config['task_type'] = json.loads(Path('data/'+dbs+'/info.json').read_text())['task_type']\n",
    "    config['cat_policy'] = json.loads(Path('data/'+dbs+'/info.json').read_text())['cat_policy']\n",
    "    config['norm'] = json.loads(Path('data/'+dbs+'/info.json').read_text())['norm']\n",
    "    dir_ = 'data/'+ dbs\n",
    "    N_train, N_test,N_val, y_train, y_test,y_val = joinData(dbs,\n",
    "                                                            cat_policy=config['cat_policy'],\n",
    "                                                            normalization=True, \n",
    "                                                            norm=config['norm'])\n",
    " \n",
    "    if config['task_type']  != 'regression':\n",
    "        bst = CatBoostClassifier(verbose=False)\n",
    "        bst.fit(N_train, y_train)\n",
    "    else :\n",
    "        \n",
    "        bst = CatBoostRegressor(verbose=False)\n",
    "        \n",
    "        bst.fit(N_train, y_train)\n",
    "        \n",
    "    y_hat_test = bst.predict(N_test)\n",
    "    # y_hat_test = (y_hat_test > 0.5).astype(int)\n",
    "    if config['task_type']  != 'regression':\n",
    "        te_acc =  precision_recall_fscore_support(y_test, y_hat_test, average='macro')\n",
    "        print(\"Test score: precision.      {}, recall {}, F1 {}, support {}\".format(te_acc[0],te_acc[1],te_acc[2],te_acc[3]) )\n",
    "    else:\n",
    "        te_acc = np.sqrt(mean_squared_error(y_test, y_hat_test)) \n",
    "        print(te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d918920d-6975-4811-b39d-f0f989eb6455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
